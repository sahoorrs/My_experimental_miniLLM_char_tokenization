{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89090b64",
   "metadata": {},
   "source": [
    "# MiniGPT Colab Notebook\n",
    "\n",
    "This notebook builds a **small GPT-style language model** from scratch (causal / autoregressive).\n",
    "\n",
    "It tries to load text from a local uploaded file **(if present)** and falls back to a public domain book URL.\n",
    "\n",
    "**Local uploaded file path included (per your session):**\n",
    "\n",
    "`/mnt/data/A_2D_digital_illustration_diagram_depicts_an_Artif.png`\n",
    "\n",
    "If the local file is not a text file, the notebook will automatically use a Gutenberg URL (Alice in Wonderland) as default.\n",
    "\n",
    "Run the cells sequentially. Adjust hyperparameters in the `Config` class to scale up/down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ad97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab-friendly)\n",
    "!pip install torch tqdm requests --quiet\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "print('Torch version:', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de69017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) Load text dataset\n",
    "# ---------------------------\n",
    "LOCAL_FILE = r\"/mnt/data/A_2D_digital_illustration_diagram_depicts_an_Artif.png\"\n",
    "GUTENBERG_URL = 'https://www.gutenberg.org/files/11/11-0.txt'  # fallback (Alice in Wonderland)\n",
    "\n",
    "text = None\n",
    "if os.path.exists(LOCAL_FILE):\n",
    "    try:\n",
    "        with open(LOCAL_FILE, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        print('Loaded local file as text from:', LOCAL_FILE)\n",
    "    except Exception as e:\n",
    "        print('Local file exists but could not be read as text (will use Gutenberg). Error:', e)\n",
    "\n",
    "if text is None:\n",
    "    print('Downloading fallback text from Gutenberg...')\n",
    "    text = requests.get(GUTENBERG_URL).text\n",
    "\n",
    "text = text.replace('\\r', '\\n')\n",
    "print('\\nSample (first 400 chars):\\n')\n",
    "print(text[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b562f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2) Tokenization (character-level for learning)\n",
    "# ---------------------------\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('Vocab size (characters):', vocab_size)\n",
    "\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print('Train tokens:', len(train_data), 'Val tokens:', len(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb28131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3) MiniGPT implementation (causal)\n",
    "# ---------------------------\n",
    "import math\n",
    "\n",
    "class Config:\n",
    "    vocab_size = len(chars)\n",
    "    block_size = 128\n",
    "    n_layer = 4\n",
    "    n_head = 4\n",
    "    n_embd = 256\n",
    "    dropout = 0.1\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "def causal_mask(T, device):\n",
    "    mask = torch.tril(torch.ones(T, T, device=device)).unsqueeze(0)\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=n_embd, num_heads=n_head, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=attn_mask)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.vocab_size = cfg.vocab_size\n",
    "        self.block_size = cfg.block_size\n",
    "        self.n_embd = cfg.n_embd\n",
    "        self.tok_emb = nn.Embedding(self.vocab_size, self.n_embd)\n",
    "        self.pos_emb = nn.Embedding(self.block_size, self.n_embd)\n",
    "        self.drop = nn.Dropout(cfg.dropout)\n",
    "        self.blocks = nn.ModuleList([GPTBlock(self.n_embd, cfg.n_head, cfg.dropout) for _ in range(cfg.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(self.n_embd)\n",
    "        self.head = nn.Linear(self.n_embd, self.vocab_size, bias=False)\n",
    "        self.head.weight = self.tok_emb.weight\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        if T > self.block_size:\n",
    "            raise ValueError(f\"Cannot forward: sequence length T={T} > block_size={self.block_size}\")\n",
    "        device = idx.device\n",
    "        tok = self.tok_emb(idx)\n",
    "        pos = self.pos_emb(torch.arange(T, device=device)).unsqueeze(0)\n",
    "        x = self.drop(tok + pos)\n",
    "        attn_mask = causal_mask(T, device=device).squeeze(0)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask=attn_mask)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            return logits, loss\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=200, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        idx = idx.to(device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            logits_last = logits[:, -1, :]\n",
    "            logits_last = logits_last / (temperature if temperature > 0 else 1.0)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits_last, min(top_k, logits_last.size(-1)))\n",
    "                min_topk = v[:, -1].unsqueeze(-1)\n",
    "                logits_last = torch.where(logits_last < min_topk, torch.full_like(logits_last, -float('Inf')), logits_last)\n",
    "            probs = F.softmax(logits_last, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d686ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4) Training loop (small demo)\n",
    "# ---------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MiniGPT(cfg).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "batch_size = 64\n",
    "block_size = cfg.block_size\n",
    "\n",
    "def get_batch(split):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "steps = 1000\n",
    "model.train()\n",
    "for step in tqdm(range(steps)):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, targets=yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step} loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac85cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) Generation example\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "prompt = 'The rabbit'\n",
    "context = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
    "out = model.generate(context, max_new_tokens=300, temperature=0.8, top_k=40)\n",
    "generated_text = decode(out[0].tolist())\n",
    "print('\\n=== Generated Text ===\\n')\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
