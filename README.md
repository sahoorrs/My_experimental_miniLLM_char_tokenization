# MINI LLM (Tiny GPT) From Scratch

* Works in Google Colab â€” requires only PyTorch.  
* Trains a small language model on a text corpus (e.g., Alice in Wonderland)  
* Load and prepare dataset  
* Creates a character tokenization for experimental learning, instead of word or subword tokenization  
* Define the Model (Input, Transformer, and output)  
* Initializes model and training settings  
* Run training loop, loss backward propagation  
* Generate Text based on input prompt

This is the raw form of LLM that gives the opportunities to change the encoding to word encoding, subword encoding and change the training setting, layers etc.  
